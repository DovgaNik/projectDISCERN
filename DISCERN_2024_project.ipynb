{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PMh_yax3zXm"
      },
      "source": [
        "Project for DISCERN 2024\n",
        "===\n",
        "\n",
        "This project is supposed to use [gpt2 model from huggingface](https://huggingface.co/openai-community/gpt2) and then use train data from [kaggle](https://www.kaggle.com/datasets/emineyetm/fake-news-detection-datasets) to detect fake and true information in news and articles. The articles used for checking collected by our team [are here](https://unirau-my.sharepoint.com/:x:/g/personal/dovhan_o_nikita22_stud_rau_ro/EVZaoVJ1OIFFmkT7ognXzbcBiR8JDDXK-ID0DWAdiFnMvg?e=DTESAz).\n",
        "\n",
        "Main inspiration taken from the [folder of this programme](https://unirau-my.sharepoint.com/personal/andrei_luchici_rau_ro/_layouts/15/onedrive.aspx?ga=1&id=%2Fpersonal%2Fandrei%5Fluchici%5Frau%5Fro%2FDocuments%2F24%2D02%2D05%20%2D%20DISCERN%20%2D%20Ed%201)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D2fLMyCix58S",
        "outputId": "f6490c88-18dc-4fbb-a862-be90a90fd5b6"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBTOh1nu0MS-",
        "ExecuteTime": {
          "end_time": "2024-02-08T21:52:51.482602100Z",
          "start_time": "2024-02-08T21:52:37.763645900Z"
        }
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import GPT2Tokenizer, TFGPT2Model, TFGPT2ForSequenceClassification\n",
        "import tensorflow as tf\n",
        "import re\n",
        "\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "import re\n",
        "from nltk.corpus import wordnet\n",
        "import nltk\n",
        "import random\n",
        "\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ciuFNAXGuEWD"
      },
      "source": [
        "Parameters\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-08T21:52:51.494436600Z",
          "start_time": "2024-02-08T21:52:51.486658400Z"
        },
        "id": "K8YFlbaXuEWE"
      },
      "outputs": [],
      "source": [
        "true_file = \"/content/drive/MyDrive/mount/True.csv\"\n",
        "fake_file = \"/content/drive/MyDrive/mount/Fake.csv\"\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "model_dir = \"3.model\"\n",
        "\n",
        "amount_of_entries = 3000\n",
        "batch_size = 8"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def augment_text(text):\n",
        "    words = text.split()\n",
        "    augmented_text = []\n",
        "    for word in words:\n",
        "        augmented_text.append(word)\n",
        "        if random.random() < 0.2:  # 20% chance for augmentation\n",
        "            synonym = find_synonym(word)\n",
        "            if synonym:\n",
        "                augmented_text[-1] = synonym\n",
        "        if random.random() < 0.1:  # 10% chance for augmentation\n",
        "            random_word = random.choice(words)\n",
        "            augmented_text.append(random_word)\n",
        "    return ' '.join(augmented_text)"
      ],
      "metadata": {
        "id": "lTfauDxT7xtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def find_synonym(word):\n",
        "    synonyms = set()\n",
        "    for syn in wordnet.synsets(word):\n",
        "        for lemma in syn.lemmas():\n",
        "            synonyms.add(lemma.name())\n",
        "    if len(synonyms) > 1:\n",
        "        return random.choice(list(synonyms - {word}))\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "XWw3uvHZ71WP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-08T21:52:52.537552600Z",
          "start_time": "2024-02-08T21:52:51.492323200Z"
        },
        "id": "VSji7860uEWF"
      },
      "outputs": [],
      "source": [
        "true_df = pd.read_csv(true_file)\n",
        "fake_df = pd.read_csv(fake_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-08T21:52:52.606098400Z",
          "start_time": "2024-02-08T21:52:52.594668100Z"
        },
        "id": "JDREI4m6uEWH"
      },
      "outputs": [],
      "source": [
        "true_df[\"label\"] = 1\n",
        "fake_df[\"label\"] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-08T21:52:52.606098400Z",
          "start_time": "2024-02-08T21:52:52.597829200Z"
        },
        "id": "HjdRTVAIuEWH"
      },
      "outputs": [],
      "source": [
        "combined_df = pd.concat([true_df, fake_df], ignore_index=True)\n",
        "\n",
        "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "combined_df = combined_df.head(amount_of_entries)"
      ]
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "combined_df['text'] = combined_df['text'].apply(lambda x: re.sub(r'^[A-Z\\s]+\\([A-Za-z\\s]+\\)\\s*-\\s*', '', x))\n",
        "combined_df['text'] = combined_df['text'].apply(lambda x: re.sub(r'[^\\w\\s]', '', x.lower()))  # Remove punctuation and lowercase\n",
        "combined_df['text'] = combined_df['text'].apply(augment_text)  # Data augmentation"
      ],
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-08T21:52:52.606098400Z",
          "start_time": "2024-02-08T21:52:52.597829200Z"
        },
        "id": "x1spRlpCuEWI"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-08T21:52:52.611145800Z",
          "start_time": "2024-02-08T21:52:52.606098400Z"
        },
        "id": "tjJB55PtuEWJ"
      },
      "outputs": [],
      "source": [
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    combined_df[\"text\"].values,\n",
        "    combined_df[\"label\"].values,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-08T21:52:53.690913Z",
          "start_time": "2024-02-08T21:52:52.613743Z"
        },
        "id": "bXd67hpWuEWK"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-08T21:52:58.875887200Z",
          "start_time": "2024-02-08T21:52:53.696874100Z"
        },
        "id": "t_LnwMxnuEWK"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=512, return_tensors='tf')\n",
        "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=512, return_tensors='tf')\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "ExecuteTime": {
          "end_time": "2024-02-08T21:53:01.546208Z",
          "start_time": "2024-02-08T21:52:58.875887200Z"
        },
        "id": "mVRTWOJtuEWL"
      },
      "outputs": [],
      "source": [
        "class GPT2Classifier(tf.keras.Model):\n",
        "    def __init__(self, num_classes, l2_lambda=0.01):\n",
        "        super(GPT2Classifier, self).__init__()\n",
        "        self.gpt2 = TFGPT2Model.from_pretrained(model_name)\n",
        "        self.dropout = tf.keras.layers.Dropout(0.5)\n",
        "        self.dense = tf.keras.layers.Dense(num_classes, activation='softmax',\n",
        "                                           kernel_regularizer=tf.keras.regularizers.L2(l2_lambda))\n",
        "\n",
        "    def call(self, inputs):\n",
        "        outputs = self.gpt2(inputs)[0]\n",
        "        pooled_output = tf.reduce_mean(outputs, axis=1)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.dense(pooled_output)\n",
        "        return logits\n",
        "\n",
        "model = GPT2Classifier(num_classes=2)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy()\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "is_executing": true,
        "ExecuteTime": {
          "start_time": "2024-02-08T21:53:01.546208Z"
        },
        "id": "ePrMeO96uEWM"
      },
      "outputs": [],
      "source": [
        "model.fit(train_dataset.shuffle(1000).batch(5), epochs=3, batch_size=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1MEcqA25uEWN"
      },
      "outputs": [],
      "source": [
        "model.save(\"/content/drive/MyDrive/mount/modelcollab5k\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lyjUsuinuEWM"
      },
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_dataset.batch(32))\n",
        "print(f\"Test accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MBGFNKlduEWN"
      },
      "source": [
        "Testing\n",
        "==="
      ]
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "def predict_news_class(probabilities, threshold=0.5):\n",
        "    fake_probability, true_probability = probabilities[0]\n",
        "    if fake_probability > threshold:\n",
        "        return \"fake\"\n",
        "    else:\n",
        "        return \"true\"\n"
      ],
      "metadata": {
        "id": "4rYZu--zuEWN"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "model = tf.keras.models.load_model(model_dir)"
      ],
      "metadata": {
        "id": "kr0l9BZIuEWO"
      },
      "execution_count": null
    },
    {
      "cell_type": "code",
      "outputs": [],
      "source": [
        "inputs = tokenizer(input(), return_tensors='tf', max_length=512, truncation=True, padding='max_length')\n",
        "output = model(inputs)\n",
        "predict_news_class(output)"
      ],
      "metadata": {
        "id": "ZMwfeTwruEWO"
      },
      "execution_count": null
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}