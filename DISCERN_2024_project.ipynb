{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PMh_yax3zXm"
      },
      "source": [
        "Project for DISCERN 2024\n",
        "===\n",
        "\n",
        "This project is supposed to use [gpt2 model from huggingface](https://huggingface.co/openai-community/gpt2) and then use train data from [kaggle](https://www.kaggle.com/datasets/emineyetm/fake-news-detection-datasets) to detect fake and true information in news and articles. The articles used for checking collected by our team [are here](https://unirau-my.sharepoint.com/:x:/g/personal/dovhan_o_nikita22_stud_rau_ro/EVZaoVJ1OIFFmkT7ognXzbcBiR8JDDXK-ID0DWAdiFnMvg?e=DTESAz)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "NBTOh1nu0MS-"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "#import tensorflow_text as text\n",
        "from transformers import GPT2Tokenizer, TFGPT2LMHeadModel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B2yVWmb00Nve",
        "outputId": "dc4a7d0c-3fd6-4f3e-f962-c2e7b2e3a3de"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "838c7301729b4882851661fac30ea206",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\winterSchoolRAUFebruary\\project\\env\\Lib\\site-packages\\huggingface_hub\\file_download.py:149: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Nikit\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40293b9d681f4b829c821809a586da32",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c3cf120526b4502aebd10de30878895",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "47947543f15e41d59973b580528075d7",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "874e627b987d4d61824c93240f2ffbb1",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\winterSchoolRAUFebruary\\project\\env\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "model = TFGPT2LMHeadModel.from_pretrained(\"gpt2\", pad_token_id=tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "5CePDrA20P9B"
      },
      "outputs": [],
      "source": [
        "input_text = \"Romanian president is going on a trip to Smolensk\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "N_v4osui0SJu"
      },
      "outputs": [],
      "source": [
        "input_ids = tokenizer.encode(input_text, return_tensors='tf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "XZ9NW7gf0ZJp"
      },
      "outputs": [],
      "source": [
        "output = model.generate(input_ids, max_length=100, num_return_sequences=1, pad_token_id=tokenizer.eos_token_id)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "jqSWCeui0adQ"
      },
      "outputs": [],
      "source": [
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "yr8QabaO0fFH",
        "outputId": "0104c5b9-67ee-408a-c19a-97bf5c61a651"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Romanian president is going on a trip to Smolensk, where he will meet with the Russian president, Vladimir Putin.\n",
            "\n",
            "The trip is part of a larger effort by the Kremlin to bolster its influence in the region.\n",
            "\n",
            "The Kremlin has been trying to boost its influence in the region since the fall of the Soviet Union.\n",
            "\n",
            "The Kremlin has been trying to boost its influence in the region since the fall of the Soviet Union.\n",
            "\n",
            "The Kremlin has been trying to boost\n"
          ]
        }
      ],
      "source": [
        "print(generated_text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_HSkhvq5H5k"
      },
      "source": [
        "TRAINING\n",
        "==="
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xcPF-RCU5OnK"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import GPT2Tokenizer, TFGPT2Model, TFGPT2ForSequenceClassification\n",
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "fMcDo6ze5RG4"
      },
      "outputs": [],
      "source": [
        "true_df = pd.read_csv(\"True.csv\")\n",
        "fake_df = pd.read_csv(\"Fake.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "cQkFDlCF5yWk"
      },
      "outputs": [],
      "source": [
        "true_df = true_df.sample(frac=1).reset_index(drop=True)  # shuffle\n",
        "fake_df = fake_df.sample(frac=1).reset_index(drop=True)  # shuffle"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sLU7Xycf5zq5"
      },
      "outputs": [],
      "source": [
        "true_df[\"label\"] = 1\n",
        "fake_df[\"label\"] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "N0LEjQXKPNsM"
      },
      "outputs": [],
      "source": [
        "num_entries_per_file = 400  # Specify the desired number of entries\n",
        "\n",
        "limited_true_df = true_df.head(num_entries_per_file)\n",
        "limited_fake_df = fake_df.head(num_entries_per_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "uLGceMMj6NFc"
      },
      "outputs": [],
      "source": [
        "combined_df = pd.concat([limited_true_df, limited_fake_df], ignore_index=True)\n",
        "combined_df = combined_df[[\"text\", \"label\"]]  # Selecting relevant columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "E2eNP_Cn6Po_"
      },
      "outputs": [],
      "source": [
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    combined_df[\"text\"].values,\n",
        "    combined_df[\"label\"].values,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "yQUMUkpn_Ilc"
      },
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "tpLOzsVY_L1q"
      },
      "outputs": [],
      "source": [
        "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=512, return_tensors='tf')\n",
        "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=512, return_tensors='tf')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "lbHy90WA6Q1x"
      },
      "outputs": [],
      "source": [
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        "))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vdVCgkDq6USG",
        "outputId": "565cc243-25e3-4196-fefb-a52b16ad105c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2Model.\n",
            "\n",
            "All the weights of TFGPT2Model were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2Model for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# Build classification model\n",
        "class GPT2Classifier(tf.keras.Model):\n",
        "    def __init__(self, num_classes):\n",
        "        super(GPT2Classifier, self).__init__()\n",
        "        self.gpt2 = TFGPT2Model.from_pretrained(\"gpt2\")\n",
        "        self.dropout = tf.keras.layers.Dropout(0.1)\n",
        "        self.classifier = tf.keras.layers.Dense(num_classes, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        outputs = self.gpt2(inputs)[0]\n",
        "        pooled_output = tf.reduce_mean(outputs, axis=1)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "model = GPT2Classifier(num_classes=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "4lGIP1f16dtr"
      },
      "outputs": [],
      "source": [
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VaYCqSRL6foo",
        "outputId": "7116e44e-6d15-483c-d95a-12e4ea65c301"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "WARNING:tensorflow:From d:\\winterSchoolRAUFebruary\\project\\env\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "d:\\winterSchoolRAUFebruary\\project\\env\\Lib\\site-packages\\keras\\src\\backend.py:5727: UserWarning: \"`sparse_categorical_crossentropy` received `from_logits=True`, but the `output` argument was produced by a Softmax activation and thus does not represent logits. Was this intended?\n",
            "  output, from_logits = _get_logits(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From d:\\winterSchoolRAUFebruary\\project\\env\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
            "\n",
            "107/107 [==============================] - 1361s 12s/step - loss: 0.4887 - accuracy: 0.8938\n",
            "Epoch 2/3\n",
            "107/107 [==============================] - 1264s 12s/step - loss: 0.0082 - accuracy: 0.9984\n",
            "Epoch 3/3\n",
            "107/107 [==============================] - 1294s 12s/step - loss: 0.0019 - accuracy: 1.0000\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x2000f38f250>"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "batch_size = 6  # Adjust this value based on your memory constraints\n",
        "\n",
        "model.fit(train_dataset.shuffle(1000).batch(batch_size), epochs=3, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "udMq6N2q6hue"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 110s 11s/step - loss: 6.4016e-04 - accuracy: 1.0000\n",
            "Test accuracy: 1.0\n"
          ]
        }
      ],
      "source": [
        "loss, accuracy = model.evaluate(test_dataset.batch(16))\n",
        "print(f\"Test accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: 1.model\\assets\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:tensorflow:Assets written to: 1.model\\assets\n"
          ]
        }
      ],
      "source": [
        "model_dir = '1.model'\n",
        "model.save(model_dir)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
