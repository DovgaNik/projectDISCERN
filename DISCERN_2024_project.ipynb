{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0PMh_yax3zXm"
      },
      "source": [
        "Project for DISCERN 2024\n",
        "===\n",
        "\n",
        "This project is supposed to use [gpt2 model from huggingface](https://huggingface.co/openai-community/gpt2) and then use train data from [kaggle](https://www.kaggle.com/datasets/emineyetm/fake-news-detection-datasets) to detect fake and true information in news and articles. The articles used for checking collected by our team [are here](https://unirau-my.sharepoint.com/:x:/g/personal/dovhan_o_nikita22_stud_rau_ro/EVZaoVJ1OIFFmkT7ognXzbcBiR8JDDXK-ID0DWAdiFnMvg?e=DTESAz)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NBTOh1nu0MS-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import GPT2Tokenizer, TFGPT2Model, TFGPT2ForSequenceClassification\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Parameters\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "true_file = \"data/True.csv\"\n",
        "fake_file = \"data/Fake.csv\"\n",
        "\n",
        "model_name = \"gpt2\"\n",
        "model_dir = \"results/2.model\"\n",
        "\n",
        "amount_of_entries = 1500\n",
        "batch_size = 10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "true_df = pd.read_csv(true_file)\n",
        "fake_df = pd.read_csv(fake_file)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "true_df[\"label\"] = 1\n",
        "fake_df[\"label\"] = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "combined_df = pd.concat([true_df, fake_df], ignore_index=True)\n",
        "combined_df = combined_df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "combined_df = combined_df.head(amount_of_entries)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "train_texts, test_texts, train_labels, test_labels = train_test_split(\n",
        "    combined_df[\"text\"].values,\n",
        "    combined_df[\"label\"].values,\n",
        "    test_size=0.2,\n",
        "    random_state=42\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "tokenizer = GPT2Tokenizer.from_pretrained(model_name)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "train_encodings = tokenizer(train_texts.tolist(), truncation=True, padding=True, max_length=512, return_tensors='tf')\n",
        "test_encodings = tokenizer(test_texts.tolist(), truncation=True, padding=True, max_length=512, return_tensors='tf')\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(train_encodings),\n",
        "    train_labels\n",
        "))\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((\n",
        "    dict(test_encodings),\n",
        "    test_labels\n",
        "))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class GPT2Classifier(tf.keras.Model):\n",
        "    def __init__(self, num_classes):\n",
        "        super(GPT2Classifier, self).__init__()\n",
        "        self.gpt2 = TFGPT2Model.from_pretrained(model_name)\n",
        "        self.dropout = tf.keras.layers.Dropout(0.6)\n",
        "        self.classifier = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        outputs = self.gpt2(inputs)[0]\n",
        "        pooled_output = tf.reduce_mean(outputs, axis=1)\n",
        "        pooled_output = self.dropout(pooled_output)\n",
        "        logits = self.classifier(pooled_output)\n",
        "        return logits\n",
        "\n",
        "model = GPT2Classifier(num_classes=2)\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=3e-5)\n",
        "loss = tf.keras.losses.SparseCategoricalCrossentropy() \n",
        "model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.fit(train_dataset.shuffle(1000).batch(batch_size), epochs=2, batch_size=batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "loss, accuracy = model.evaluate(test_dataset.batch(32))\n",
        "print(f\"Test accuracy: {accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.save(model_dir)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
